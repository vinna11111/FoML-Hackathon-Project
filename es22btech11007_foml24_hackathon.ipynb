{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "685b0ba8-6976-4e6b-ab51-c0b91bc37779",
   "metadata": {},
   "source": [
    "Loading all the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "19b065d2-b3e7-4607-afd9-c5ef3d699927",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa915bbb-546f-48d1-965a-c9ac04aba18c",
   "metadata": {},
   "source": [
    "loading data to store median and mode values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e442c0f0-f399-4f22-9e95-0d069c0ddc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data = pd.read_csv('train.csv')\n",
    "original_data = original_data.drop(['UID', 'Target'], axis=1)\n",
    "# Store the mode of specific categorical columns which i have identified from the data\n",
    "\n",
    "categorical_cols = ['SoilFertilityType', 'TypeOfIrrigationSystem', 'CropFieldConfiguration',\n",
    "                       'FarmClassification', 'HarvestProcessingType', 'LandUsageType','DistrictId','NationalRegionCode']\n",
    "mode_values = {col: original_data[col].mode()[0] for col in categorical_cols}\n",
    "\n",
    "# Store the median values of each column from original_data\n",
    "median_values = original_data.median()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac89bd2-f11f-4985-8be2-aacf23d908e3",
   "metadata": {},
   "source": [
    "here is function which creates additional colums which helps us to improvw the efficiency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "65baa187-0d1a-408b-9913-150d50e9ce74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_additional_features(df):\n",
    "    \"\"\"Create more sophisticated engineered features based on domain knowledge\"\"\"\n",
    "    data = df.copy()\n",
    "    \n",
    "    # 1. Water-related ratios and interactions\n",
    "    data['water_density'] = data['WaterAccessPoints'] / data['FieldSizeSqft']\n",
    "    data['water_per_cultivated'] = data['WaterAccessPoints'] / data['TotalCultivatedAreaSqft']\n",
    "    data['reservoir_density'] = data['WaterReservoirCount'] / data['FieldSizeSqft']\n",
    "    \n",
    "    # 2. Cultivation efficiency metrics\n",
    "    data['cultivation_ratio'] = data['TotalCultivatedAreaSqft'] / data['FieldSizeSqft']\n",
    "    data['greenhouse_density'] = data['NumberGreenHouses'] / data['FieldSizeSqft']\n",
    "    data['farming_intensity'] = data['FarmingUnitCount'] / data['TotalCultivatedAreaSqft']\n",
    "    \n",
    "    # 3. Infrastructure utilization\n",
    "    data['storage_per_area'] = (data['UndergroundStorageSqft'] + data['HarvestStorageSqft']) / data['FieldSizeSqft']\n",
    "    data['equipment_ratio'] = data['FarmEquipmentArea'] / data['FieldSizeSqft']\n",
    "    \n",
    "    # 4. Economic indicators\n",
    "    data['value_per_sqft'] = data['TotalValue'] / data['FieldSizeSqft']\n",
    "    data['tax_burden'] = data['TotalTaxAssessed'] / data['TotalValue']\n",
    "    \n",
    "    # 5. Operational scale indicators\n",
    "    data['vehicle_per_area'] = data['FarmVehicleCount'] / data['TotalCultivatedAreaSqft']\n",
    "    data['irrigation_coverage'] = data['MainIrrigationSystemCount'] / data['TotalCultivatedAreaSqft']\n",
    "    \n",
    "    # 6. Field age and development\n",
    "    current_year = 2024\n",
    "    data['field_age'] = current_year - data['FieldEstablishedYear']\n",
    "    \n",
    "    # 7. Location-based features\n",
    "    data['location_cluster'] = data['Latitude'].astype(str) + '_' + data['Longitude'].astype(str)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef45733-f711-43ea-b57e-bd1cfabd660a",
   "metadata": {},
   "source": [
    "here is the function which preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f4265210-5f32-40ec-9a2d-71659521abab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df, is_train=True):\n",
    "    \n",
    "    data = df.copy()\n",
    "    \n",
    "    categorical_cols = ['SoilFertilityType', 'TypeOfIrrigationSystem', 'CropFieldConfiguration',\n",
    "                       'FarmClassification', 'HarvestProcessingType', 'LandUsageType','DistrictId','NationalRegionCode']\n",
    "      #1. Handle categorical variables\n",
    "    for col in categorical_cols:\n",
    "        if data[col].isnull().sum() > 0:\n",
    "            # Use mode for categorical variables\n",
    "            data[col] = data[col].fillna(mode_values[col])\n",
    "      # 2.Handle numerical variables\n",
    "        \n",
    "    numeric_cols = data.select_dtypes(include=['float64', 'int64']).columns\n",
    "    for col in numeric_cols:\n",
    "        if data[col].isnull().sum() > 0:\n",
    "            if col in ['WaterAccessPoints', 'WaterAccessPointsCalc']:\n",
    "                # Use nearest neighbor interpolation for water-related features\n",
    "                data[col] = data[col].interpolate(method='nearest')\n",
    "            elif 'Area' in col or 'Sqft' in col:\n",
    "                # Use linear interpolation for area-related features\n",
    "                data[col] = data[col].interpolate(method='linear')\n",
    "            else:\n",
    "                # Use median for other numeric features\n",
    "                data[col] = data[col].fillna(median_values[col])\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    # 3. Create advanced features\n",
    "    data = create_additional_features(data)\n",
    "    \n",
    "    # 4. Encode categorical variables\n",
    "    categorical_cols = data.select_dtypes(include=['object']).columns\n",
    "    label_encoders = {}\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        if col not in ['UID', 'Target']:\n",
    "            le = LabelEncoder()\n",
    "            data[col] = le.fit_transform(data[col].astype(str))\n",
    "            label_encoders[col] = le\n",
    "    \n",
    "    # 5. Scale numeric features\n",
    "    numeric_cols = data.select_dtypes(include=['float64', 'int64']).columns\n",
    "    numeric_cols = [col for col in numeric_cols if col not in ['UID', 'Target']]\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    data[numeric_cols] = scaler.fit_transform(data[numeric_cols])\n",
    "    \n",
    "    # 6. Handle outliers using IQR method\n",
    "    for col in numeric_cols:\n",
    "        Q1 = data[col].quantile(0.25)\n",
    "        Q3 = data[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        data[col] = data[col].clip(lower_bound, upper_bound)\n",
    "    \n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998713fd-4f90-4b66-842f-df05621923fa",
   "metadata": {},
   "source": [
    "here is the model model which we have built"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86020fd-9f08-4f1a-8661-b7760d0aa4dd",
   "metadata": {},
   "source": [
    "the hyperparameters used are obtained through grid search cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c8dad7f2-a86e-4020-b693-8e60d18ad238",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X, y):\n",
    "    \n",
    "    # Calculate class weights\n",
    "    classes = np.unique(y)\n",
    "    class_weights = compute_class_weight('balanced', classes=classes, y=y)\n",
    "    class_weights_dict = {i: weight for i, weight in zip(classes, class_weights)}\n",
    "    \n",
    "    # Sample weights based on class weights\n",
    "    sample_weights = np.array([class_weights_dict[label] for label in y])\n",
    "    \n",
    "    # Define CPU-optimized XGBoost model configuration\n",
    "    model = xgb.XGBClassifier(\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=6,\n",
    "        min_child_weight=1,\n",
    "        gamma=0.5,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        tree_method='hist',\n",
    "        max_bin=256,\n",
    "        grow_policy='lossguide',\n",
    "        eval_metric=['mlogloss', 'merror'],\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Fit model on the entire dataset\n",
    "    model.fit(\n",
    "        X, y,\n",
    "        sample_weight=sample_weights,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20729098-b217-4593-841e-50188f4ca864",
   "metadata": {},
   "source": [
    "here is the data which will preprocess,train,and store the predictions in submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1d473fa1-4f06-44a2-bd92-00c1cc308786",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train.csv\")\n",
    "\n",
    "mapping = {'low': 0, 'medium': 1, 'high': 2}\n",
    "\n",
    "# Apply this mapping to the target column\n",
    "train_df['Target'] = train_df['Target'].map(mapping)\n",
    "    \n",
    "# Preprocessing the data\n",
    "train_processed = preprocessing(train_df, is_train=True)\n",
    "    \n",
    "# Preparing features and target\n",
    "X = train_processed.drop(['UID', 'Target'], axis=1)\n",
    "y = train_processed['Target']\n",
    "    \n",
    "# Train the model with entire train data\n",
    "model_with_total_data = train_model(X, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0a0183d3-187a-4c22-8db0-feee40dd3cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(test_fname, predictions_fname):\n",
    "    test_df=test_fname\n",
    "    test_processed = preprocessing(test_df, is_train=False)\n",
    "    test_X = test_processed.drop(['UID'], axis=1)\n",
    "    predictions = model_with_total_data.predict(test_X)\n",
    "    reverse_mapping = {0: 'low', 1: 'medium', 2: 'high'}\n",
    "    predictions = [reverse_mapping[label] for label in predictions]\n",
    "    submission = pd.DataFrame({\n",
    "    'UID': test_df['UID'],\n",
    "    'Target': predictions\n",
    "    })\n",
    "    submission.to_csv(predictions_fname, index=False)\n",
    "    # Read a test set from the file test_fname (which will be in the same format as\n",
    "    # test.csv) and write to a submission file predictions_fname.csv\n",
    "    # You are not required to return anything, you will save your predictions in the same \n",
    "    # format as sample_submission.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bb63ef-0319-4258-8477-3b422b21d3aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
